\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage[inkscapeformat=png]{svg}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{PereFlex: A tool for automated evaluation of error recovery in parsers\\
}

\author{
\IEEEauthorblockN{Olga Bachishche}
\IEEEauthorblockA{\textit{ITMO University}\\
Saint-Petersburg, Russia\\
bachisheo@yandex.ru}
\and
\IEEEauthorblockN{Yaroslav Vorobiev}
\IEEEauthorblockA{
\textit{HSE University}\\
Saint-Petersburg, Russia \\
yaroslav.vorobev-2015@mail.ru}
\and
\IEEEauthorblockN{Grigory Raykin}
\IEEEauthorblockA{
\textit{ITMO University}\\
Saint-Petersburg, Russia \\
gregra@mail.ru}

\linebreakand\IEEEauthorblockN{Daria Vasina}
\IEEEauthorblockA{
\textit{ITMO University}\\
Saint-Petersburg, Russia \\
dashavasina625@gmail.com}
\and
\IEEEauthorblockN{Daniil Shushakov}
\IEEEauthorblockA{
\textit{ITMO University}\\
Saint-Petersburg, Russia \\
shushakov4@ya.ru}
\and
\IEEEauthorblockN{Semyon Grigorev}
\IEEEauthorblockA{\textit{Saint Petersburg State University} \\
Saint-Petersburg, Russia \\
s.v.grigoriev@spbu.ru}
}
\maketitle

\begin{abstract}

Error recovery is a critical component of parsing technology, particularly in applications such as IDE and compilers, where a single syntax error should not prevent further analysis of the input. This paper presents PereFlex --- a tool for extensive experimental evaluation of error recovery in JVM-based parsers. Our evaluation is based on a real-world parsers for Java and users erroneous programs. The results demonstrate that while some strategies are fast, they often fail to provide meaningful recovery, whereas advanced methods offer better recovery quality at the cost of increased computational overhead.

\end{abstract}

\begin{IEEEkeywords}
error recovery, parsing, IDE, evaluation
\end{IEEEkeywords}

\section{Introduction}
%common
An integrated development environment (IDE) is a tool designed to assist developers in writing code efficiently. The features and inspections provided by an IDE are largely based on the abstract syntax tree (AST), which serves as a structural representation of the source code generated by a parser or syntax analyzer. During the coding process, programmers do not always maintain syntactically correct code.

To work effectively under such circumstances, IDE parsers must be capable of recovering from errors and generating an AST that is, if not entirely accurate, at least a reasonable approximation of the correct structure. Efficient error recovery ensures that minor syntax errors do not interrupt the development process~\cite{ide-errors}, enabling features such as autocompletion, real-time syntax highlighting, and quick fixes to function smoothly. 

Evaluation of error recovery is important from both scientific and practical standpoints. Parser developers need reliable benchmarks and metrics to compare new recovery algorithms against existing solutions. Likewise, practitioners who build tools such as IDEs, linters, or static analyzers must be able to choose a parser that meets their needs. This requires access to measurable, reproducible information about how well a parser handles erroneous input. Without a clear evaluation framework, it is difficult to make informed decisions or track progress in error recovery techniques.

Despite its practical importance, evaluating the quality of error recovery remains a challenging and largely open problem. First, there is no standardized metric for measuring recovery quality. Different approaches employ a variety of techniques, including analysis of error type distributions~\cite{error-frequence}, exact match metrics~\cite{ai-recovery-large}, and manual assessment~\cite{forward-move, natural_recovery}. Second, not all metrics are universally applicable. Some are suited to specific parsing algorithms~\cite{panic}, while others depend on the structure of the recovered AST~\cite{fuzz_recovery, natural_recovery}. Different parsers often produce different ASTs for the same incorrect input. This variation arises from differences in their grammars, error recovery strategies, and design decisions. As a result, directly comparing parsers becomes a challenging task.

In addition to metric design, dataset construction presents another fundamental challenge. Many prior studies rely on synthetic datasets, fully generated or partially modified by mutators~\cite{natural_recovery, fuzz_recovery, ai_recovery}. These datasets make it possible to automatically label error types and run controlled experiments. However, they can create error patterns that do not reflect real-world code, which limits how well the results apply in practice. To address this, recent studies~\cite{panic, ai_recovery, ide-errors} have started using datasets of real code written by developers~\cite{blackbox}. These datasets provide more realistic testing conditions and lead to more reliable evaluations.

This work addresses these challenges by introducing a unified way to evaluate parser error recovery. The proposed method does not depend on a specific parsing technique or AST representation and can be applied to a wide range of real-world codebases. 

To ensure an efficient and systematic evaluation, we first review existing approaches to assessing error recovery quality.  Building upon this foundation, we present a novel tool that automatically evaluates the error recovery quality of parsers targeting the JVM platform. Notably, the tool can operate without access to a parserâ€™s internal structure, including its parsing algorithm or specific error representations.
 
Additionally, we analyze the error recovery capabilities of widely used JVM-based parsers integrated into  IDEs such as VS Code and Eclipse. To facilitate benchmarking, we have  annotated a real-user dataset~\cite{blackbox} of Java source files containing syntactic errors. 


This research contributes to the field of parsing and syntax analysis in the following ways.
\begin{itemize}
    \item \textbf{Evaluation Metrics}: We review existing approaches to error recovery evaluation and summarize their advantages and limitations.
    \item \textbf{Tool Implementation}: We present \textbf{PereFlex}\footnote{PereFlex source code: \url{https://github.com/dsult/parser-compare}} (\textbf{Flex}ible \textbf{P}arser \textbf{E}rror \textbf{R}ecovery \textbf{E}valuation), a tool that computes a subset of these metrics for any given JVM-based parser without requiring internal access to its implementation.
    \item \textbf{Analysis of Real-World Parsers}: We apply PereFlex to evaluate both generated and standalone parsers used in real-world development environments, including Visual Studio Code\footnote{Visual Studio Code code editor: \url{https://code.visualstudio.com/}} and Eclipse\footnote{Eclipse IDE for Java: \url{https://eclipseide.org/}}, providing insights into their recovery quality and limitations. 
\end{itemize}



\section{Evaluation Methodology}

Many techniques exist for evaluating error recovery~\cite{recovery-types}. We limit our scope to parser recovery in general, excluding lexer-level errors.

\subsection{Quality Metrics}

The central question in quality assessment is how closely the recovered input matches the original user input or a correct reference version. Several evaluation strategies are widely used.

\begin{enumerate}
    \item The simplest approach involves \textbf{manual evaluation}~\cite{forward-move, natural_recovery}. However, this method is time-consuming and prone to subjective bias. Additionally, developers may be influenced by error-messages output from the tools used during verification~\cite{panic}. Despite its limitations, manual inspection remains valuable method, particularly in cases where no baseline file with corrected errors is available. 
    
    To enhance precision, some studies classify recovery quality using coarse categories such as ``excellent'', ``good'', and ``poor''~\cite{forward-move}. 
    However, this classification lacks granularity and does not allow detailed analysis.

    \item \textbf{AST comparison} assesses structural similarity between recovered and expected syntax trees~\cite{natural_recovery, fuzz_recovery}. This method is accurate but sensitive to AST format differences and parser-specific representations. This method also requires a dataset with reference recovered code to compare trees in same format.

    \item \textbf{Edit distance}\cite{editing-distance} quantifies the number of modifications needed to transform one string into another. In error recovery algorithms, particularly in syntax analysis, edit distance is employed to identify the optimal recovery path with minimal data loss\cite{panic}. While useful, it may not fully reflect structural differences in complex codebases, requiring careful application. 

    \item \textbf{Exact match}~\cite{exact-match-ai} is 1 if the recovered code exactly matches the marked target code, and 0 otherwise. Because it is based on a labeled dataset, false positives may occur when performing alternative but syntactically correct fixes. 
    
    % In the case where the parser can make several different syntactically correct reconstructions, the metric can be changed to verify that one of the recovered code variants corresponds to the target.

    \item \textbf{Cascade errors}, where one error triggers additional parsing failures, are measured by counting total error locations~\cite{panic}. Yet, fewer errors do not always imply better recovery, as some parsers may miss detecting them entirely.

    \item \textbf{First-error stopping}~\cite{first-errors, error-frequence} limits recovery assessment to the initial error, ignoring parser behavior for subsequent code, which is vital in IDE scenarios.

    \item \textbf{Error type distribution}~\cite{error-frequence} reflects the frequency and kinds of errors encountered. Though often mixing compile-time and parsing errors for Java code, it reveals patterns that highlight parser weaknesses.
\end{enumerate}

In our approach, we selected \textbf{edit distance} and \textbf{error distribution} as our primary evaluation metrics.  Edit distance provides a general, language-independent measure of how well the recovered input matches the original one, while error distribution highlights systematic flaws in recovery strategies, providing insight into the parserâ€™s robustness and practical utility.


\subsection{Performance and Memory Usage }
The most straightforward way to evaluate recovery performance and memory consumption is to measure the difference between parsing benchmarks for a correct file and the same file containing errors~\cite{natural_recovery, fuzz_recovery}. This approach eliminates common overhead costs but is only applicable to generated datasets where an ideal solution is known.  When working with real-world datasets, performance trends must be compared across different parsers rather than against a predefined baseline.

In some cases, performance measurement can be  further refined. For instance, if recovery is a distinct module  within a parsing algorithm, its execution time can be measured separately~\cite{panic}. However, this is only applicable to parsers with dedicated recovery modules.

\subsection{JVM-specific aspects of evaluation}
In this study, we analyze the performance of the JVM (Java Virtual Machine). Our methodology is based on ``Pro.NET Benchmarking''~\cite{bench}, which provides an approach to experiment design, statistical analysis, and bias mitigation that can be applied to any virtual machine.


\begin{itemize}
    \item \textbf{Use release builds with optimizations}: this ensures that benchmarks reflect real-world tool performance.
    \item \textbf{Prevent compiler optimizations from eliminating measurements}: partial results should be stored in variables to ensure correctness.
    \item \textbf{Repeat benchmarks multiple times}: each performance measurement should be executed at least 30 times, with 5â€“10 warm-up iterations to stabilize results.
    \item \textbf{Trigger garbage collection (GC) between measurements}: since GC can introduce variability, forcing a collection cycle before measurement ensures consistency.
\end{itemize}

Since this study focuses on evaluating JVM-based parsers, memory measurement must account for JVM-specific constraints. The most critical metric is peak memory usage during parsing. JVM-based programs do not have a fixed point where peak memory usage is guaranteed, as GC can reclaim memory at any time.



One possible solution is to use specialized profiling libraries such as JMH\footnote{JMH source code: \url{https://github.com/openjdk/jmh}}. However, these tools are designed for microbenchmarking and are not well suited for experiments involving large datasets or extensive computations. An alternative approach is to constrain the JVM's maximum memory allocation and use binary search to determine the minimum required memory for processing a given file.

In summary, a tool for reliable parser evaluation on the JVM requires awareness of its runtime behaviors, especially GC and JIT optimizations. 


\section{Error recovery analysis tool for JVM platform}
PereFlex provides an efficient and structured way to benchmark error recovery capabilities across different parsers on the JVM.\@
It operates as a console application, enabling users to evaluate multiple parsers for different programming languages with precise control over execution parameters.

\subsection{System Architecture}
The high-level architecture of the benchmarking tool consists of the following key components shown in Figure~\ref{fig:architecture}.

\begin{figure*}[htbp]
\includesvg[scale=0.46]{data/tool.svg}
\caption{High-level architecture of the benchmarking tool.}\label{fig:architecture}
\end{figure*}

\begin{itemize}
    \item \textbf{RecoveryAnalyzer:} An API for evaluating any JVM parser in a benchmark system and its implementation for different parsers.
    
    \item \textbf{Benchmark Runners:} Kotlin module that performs multiple measurement algorithms with a given implementation of \texttt{IRecoveryAnalyzer} and dataset. This module can be extended by users.
    
    \item \textbf{Data Representations:} Python library processes the collected benchmark data. This module generates graphs for a specific parser or compares graphs for all analyzers and calculates some statistics to obtain test results.
    
\end{itemize}

\subsection{Key Features}
The benchmarking tool currently implements the following evaluation metrics for error recovery in parsers.

\begin{itemize}
    \item \textbf{Error Types:} Categorization and analysis of errors encountered during parsing.
    \item \textbf{Similarity:} Similarity score is calculated between recovered output and expected result.  
    In current implementation original token sequence is used as the expected result.  
    The system can be extended to use a reference-fixed version, enabling broader dataset evaluation and experiment design flexibility.

   
    \item \textbf{JVM Heap Memory Usage} Tracks peak JVM-heap memory consumption during parsing.
    \item \textbf{Processing Time:} Execution time is measured multiple times for each benchmarked input. Each measurement is recorded, allowing users to specify the number of warm-up runs and actual executions even after evaluation.

\end{itemize}

The system is designed to be extensible, allowing users to define and integrate their own evaluation metrics. By following the existing benchmarking framework, users can implement custom analyzers to gather additional insights, such as parser-specific error handling strategies, CPU usage, or other performance indicators. This flexibility makes the tool adaptable for different research needs and evolving parsing technologies.


\subsection{Conclusion}
The evaluation tool described in this paper is a practical solution for researchers and developers working on JVM-based parsing. The tool is also scalable, allowing users to add their own JVM-based parsers and define custom statistical measurements, making it adaptable for different research needs and parser implementations.


\section{Experiment Design}
This section describes our approach to measuring the quality of error recovery in parsers.

\subsection{Dataset}
To evaluate the quality of error recovery, we use the real-world dataset from Blackbox, a repository of events from the BlueJ Java IDE~\cite{bluej}. This dataset has previously been used for experiments in the field of syntactic analysis\cite{panic,ai_recovery, ide-errors}, enabling direct comparisons with prior research.

\subsection{Research Questions}
We design experiments to address the following research questions.

\textbf{RQ1:} How does parsing implementation affect error detection quality?

\textbf{RQ2:} Does similarity score reflect the quality of error recovery?

\subsection{Evaluation Metrics}
In this study, we adopt \textbf{edit distance} and \textbf{error type distribution} as the principal evaluation metrics. 

The \textbf{edit distance} (\texttt{ED}) is zero for identical code; however, in our approach, we invert this metric so that the similarity score equals zero if the token sequences are completely different~\eqref{eq}.

\begin{equation}
 1 - \left( \frac{\text{ED}(\text{original}, \text{recovered})}{\max(|\text{original}|, |\text{recovered}|)} \right)\label{eq}
\end{equation}

 The \texttt{original} token sequence is obtained from the lexer phase, while the \texttt{recovered} token sequence is collected from parsing results, including error tokens if error nodes appear.


To evaluate the quality of error recovery by \textbf{error type distribution}, the dataset must be annotated with error types that should be detected. Since the target language is Java, we use the \texttt{javac}\footnote{javac: \url{https://docs.oracle.com/javase/8/docs/technotes/guides/javac/}} compiler as a reference. \texttt{javac} is part of the Java Development Kit (JDK) so it serves as the de facto standard for compiling Java source files. It provides a well-defined set of diagnostics\footnote{javac diagnostics: \\ \url{https://openjdk.org/groups/compiler/doc/hhgtjavac/diagnostics.html}} with human-readable error descriptions\footnote{javac errors: \url{https://github.com/openjdk/jdk/blob/master/src/jdk.compiler/share/classes/com/sun/tools/javac/resources/compiler.properties}}.

A key challenge is that \texttt{javac} does not strictly separate parsing and compilation errors. For instance, a parser error like \texttt{illegal start of expression} and a compilation error such as \texttt{classes: \{0\} and \{1\} have the same binary name} are defined in the same file and referenced by property names. To ensure accurate evaluation of parsing errors, we manually exclude errors from later compilation stages. As \texttt{javac} is our reference, the metric for error recovery is the number of errors detected by \texttt{javac} that were missed by other parsers.

\subsection{Implementation details}
All parser metric evaluations are abstracted using the \texttt{IRecoveryAnalyzer} interface, which provides the following methods.
\begin{itemize}
    \item \texttt{getLexerTokens} --- returns the set of original tokens obtained after lexing.
    \item \texttt{getParserTokens} --- computes the set of tokens after error recovery.
    \item \texttt{measureParseTime} --- performs parsing once and returns the execution time.
\end{itemize}

To support a custom JVM-based parser, it is necessary to implement these methods. During this research, we developed implementations for commonly used parsers and parser generators.

\begin{itemize}
    \item \textbf{Tree-sitter}\footnote{Tree-sitter parser generator: \url{https://tree-sitter.github.io/tree-sitter/}} --- A widely used parser generator, notably in VSCode. Originally written in C, it includes optional JVM bundling\footnote{Java bundle for Tree-sitter: \url{https://github.com/tree-sitter/tree-sitter-java}}.  Due to its non-heap memory usage, JVM-based memory profiling is not representative, but parsing speed and quality evaluations remain applicable. Since it lacks a built-in lexer API, we implemented an external lexer (based on JFlex for Java) for similarity distance calculation.

    \item Two versions of \textbf{ANTLR}\cite{all-star} were generated for Java:
    \begin{itemize}
        \item \textbf{ANTLR-Java8-spec} --- Based on the official Java 8 specification\footnote{ANTLR grammar based on Java 8 specification:\\ \url{https://github.com/antlr/grammars-v4/tree/master/java/java}}.
        \item \textbf{ANTLR-Java} --- Based on an optimized Java grammar\footnote{Optimized ANTLR grammar for Java:\\ \url{https://github.com/antlr/grammars-v4/tree/master/java/java8}}.
    \end{itemize}  
 

    \item \textbf{JDT} (Java Development Tools) --- Used in Eclipse, JDT differs from conventional parsers by storing keyword tokens (e.g., \texttt{if, else, catch, public}) as AST node fields instead of leaves. Extracting token sequences for comparison requires a custom AST traversal mechanism. Fortunately, JDT provides the \texttt{NaiveASTFlattener} visitor, which traverses the AST, applies error recovery strategies, and reconstructs the correct code.
\end{itemize}


In approbation purposes, we analyzed the parsing speed for each parser on the BlackBox dataset. The evaluation results are shown in Figure~\ref{fig:fig-speed}. This demonstrates that our tool enables researchers to compare different parsers in a benchmarking ecosystem with minimal overheads.

\begin{figure}[htbp]
\includesvg[scale=0.21]{data/time.svg}
\caption{Parsing time measurement on the BlackBox dataset.}\label{fig:fig-speed}
\end{figure}

\section{Evaluation}
In this section, we present the evaluation results and address our research questions.


\textbf{RQ1:} How does parsing implementation affect error detection quality?

To assess the impact of parsing implementation on error detection quality, we compare different parsers in terms of missed error messages relative to the reference parser from \texttt{javac} compiler. Figure~\ref{fig:error_distribution} presents the distribution of missed errors across multiple parsing implementations. 

\begin{figure}[htbp]
\includesvg[scale=0.22]{data/errors.svg}
\caption{Missing error distribution in comparing with javac.}\label{fig:error_distribution}
\end{figure}

The results reveal differences in error detection effectiveness. The most notable finding is the variation in the number of missed errors, particularly for common syntax issues such as \texttt{not a statement} and most common Java error\cite{error-frequence} \texttt{`;' expected}. Tree-sitter, for instance, exhibits a high rate of missed \texttt{variable declaration not allowed here} and \texttt{class, interface, enum, or record expected}  errors, indicating its limitations in handling incorrect declarations. ANTLR-Java and ANTLR-Java8-spec, while similar in parsing algorithm, show discrepancies in handling punctuation-related errors such as missing colons and parentheses. JDT, by contrast, appears to have more balanced performance but still misses certain structural errors.

These differences suggest that parsing strategies play a crucial role in error detection quality. Parsers optimized for flexibility, such as Tree-sitter or Antlr-java, may allow faster parsing but at the cost of missing critical syntactic violations. Conversely, stricter parsers such as ANTLR-Java8-spec can enforce the Java grammar more rigorously, but as we can see in Figure~\ref{fig:fig-speed}, they may struggle with parsing performance.


In summary, the choice of parsing implementation directly affects error detection quality. The trade-off between strictness and recovery flexibility determines whether a parser will correctly diagnose syntax issues or silently fail to recognize them. 

\textbf{RQ2:} Does similarity score reflect the quality of error recovery?  

The similarity score, based on edit distance, measures how closely the recovered code matches the original input. We measured this metric using a BlackBox dataset to evaluate error recovery, as shown in Figure~\ref{fig:sim}. To better understand the quality of the similarity score, we compared its results with other metrics, such as performance and error distribution. 

\begin{figure}[htbp]
\includesvg[scale=0.285]{data/sim_1.svg}
\includesvg[scale=0.285]{data/sim_2.svg}
\caption{Similarity score for Blackbox dataset.}\label{fig:sim}
\end{figure}

Our analysis showed that although the similarity score provides a useful quantification, it does not always reflect the true quality of error recovery.
\begin{itemize}
    \item \textbf{ANTLR-Java} and \textbf{Tree-sitter} show high similarity scores, but they tend to miss many syntax errors, as seen in Figure~\ref{fig:error_distribution}. Their outputs look close to the original, yet they often fail to identify and correct issues.
    
    \item \textbf{JDT-JLS21} has lower similarity scores because it carefully prunes incorrect subtrees during recovery. This leads to more edits but ensures that invalid constructs are not preserved.
    
    \item \textbf{ANTLR-Java8-spec} closely follows the Java language specification and detects many errors, resulting in more differences from the original and thus lower similarity scores.
\end{itemize}

In conclusion, while the similarity score alone is not sufficient to assess recovery quality, it still offers valuable insights. It is simple to compute, enables easy comparison between parsers, and it indicates how the recovery process is performed in general. However, since high similarity may also result from ignoring errors rather than fixing them, it must be used alongside correctness and structural validation metrics to accurately evaluate recovery behavior.


\section*{Conclusion}

In this work, we surveyed and analyzed existing approaches to measuring error recovery quality in parsers. To support subset of this analysis, we developed an open-source tool that enables researchers to evaluate both performance and recovery quality benchmarks for any JVM-based parser. The tool features an extensible architecture, allowing additional benchmarks and data analysis modules to be integrated using a consistent API.\@

Using this framework, we evaluated a real-world parser on actual erroneous user input. Our results demonstrate that parser recovery quality cannot be fully understood using a single metric alone. Instead, a comprehensive analysis across multiple metrics is necessary to draw meaningful conclusions.

This multifaceted evaluation approach allows not only for a deeper understanding of newly developed parsers but also facilitates practical comparisons between existing ones. Such comparisons can guide the selection of parsers best suited for real-world applications, depending on whether performance or error recovery is a higher priority for the end user.

\section{Future Work}

\textbf{Impact on Performance}: Understanding the impact of error recovery on performance requires proper benchmarking. Establishing a baseline for each example allows us to isolate whether performance variations stem from error recovery or differences in the parsing algorithm.

\textbf{Cross-Language Evaluation}: Extending our framework to other programming languages, particularly those using Parsing Expression Grammars (PEGs)\cite{peg-recovery, towards-recovery}, would provide broader insights into recovery strategies and language-specific challenges.

\textbf{AI-Powered Recovery}: AI-assisted tools demonstrate promising results in error recovery by analyzing the AST and suggesting fixes\cite{ai_recovery, seq2parse, ai-c-recovery}. Future work could explore integrating AI-driven recovery into JVM-based parsers and comparing its quality and effectiveness with traditional methods.

\textbf{Evaluate more real-world IDE parsers}: Some parsers used in real-world projects are not standalone tools but are instead tightly integrated components of larger systems. A key engineering challenge in benchmarking such parsers lies not in supporting them within the benchmarking framework, but in isolating and executing them independently of the full application. Notable examples include the parser from the IntelliJ IDEA IDE and the one embedded in the \texttt{javac} Java compiler.



\bibliographystyle{IEEEtran}
\bibliography{references}
\end{document}
